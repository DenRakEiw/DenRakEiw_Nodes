# ğŸš€ Advanced Latent Upscaler Training V2.0

Ein komplettes, verbessertes Training-System fÃ¼r hochqualitative Latent-Upscaler mit modernsten Deep Learning Techniken.

## ğŸ¯ Verbesserungen gegenÃ¼ber V1.0

### âœ¨ **Neue Features:**
- **ğŸ§  Residual Architecture** - Besserer Gradient Flow
- **ğŸ‘ï¸ Perceptual Loss** - Realistischere Ergebnisse  
- **ğŸ“Š GroÃŸe Datasets** - DIV2K + Flickr2K (2000+ Bilder)
- **ğŸ”„ Data Augmentation** - Flips, Rotationen, Noise
- **ğŸ“ˆ Progressive Training** - Cosine Annealing LR
- **ğŸ“Š Monitoring** - Plots, Logs, Checkpoints
- **âš¡ Auto-Setup** - Ein-Klick Installation

### ğŸ—ï¸ **Architektur-Verbesserungen:**
- **Residual Blocks** statt einfache Convolutions
- **LeakyReLU** statt Tanh fÃ¼r weniger GlÃ¤ttung
- **PixelShuffle** fÃ¼r besseres Upsampling
- **Gradient Clipping** fÃ¼r stabiles Training
- **AdamW Optimizer** mit Weight Decay

## ğŸš€ Quick Start

### **Option 1: Ein-Klick Training**
```bash
python quick_start_training.py
```

### **Option 2: Manuelles Training**
```bash
# 1. Dataset vorbereiten
python dataset_preparation.py

# 2. Training starten
python train_advanced_upscaler.py
```

## ğŸ“Š Datasets

### **Automatisch heruntergeladen:**
- **DIV2K Dataset** (800 Training + 100 Validation)
- **Sample Images** von Unsplash
- **Automatische VAE-Kodierung** zu Latents

### **UnterstÃ¼tzte Formate:**
- JPG, PNG, BMP, TIFF
- Automatische GrÃ¶ÃŸenanpassung auf 512x512
- VAE-Kodierung zu 4x64x64 Latents

## ğŸ—ï¸ Architektur

```python
AdvancedLatentUpscaler(
    input_channels=4,      # VAE Latent Channels
    output_channels=4,     # VAE Latent Channels  
    num_residual_blocks=8  # Anzahl Residual Blocks
)
```

### **Netzwerk-Flow:**
```
Input [4x32x32] 
    â†“
Initial Conv [64 channels]
    â†“
8x Residual Blocks [64 channels]
    â†“
PixelShuffle Upsampling [2x]
    â†“
Final Conv [4 channels]
    â†“
Output [4x64x64]
```

## ğŸ¯ Loss Function

**Kombinierte Loss:**
```python
total_loss = 0.7 * MSE_loss + 0.3 * Perceptual_loss
```

- **MSE Loss**: Pixel-genaue Rekonstruktion
- **Perceptual Loss**: VGG19-basierte Feature-Ã„hnlichkeit

## âš™ï¸ Training Configuration

```python
config = {
    # Model
    'num_residual_blocks': 8,
    
    # Training  
    'epochs': 200,
    'batch_size': 16,
    'learning_rate': 1e-4,
    'weight_decay': 1e-4,
    
    # Loss weights
    'mse_weight': 0.7,
    'perceptual_weight': 0.3,
    
    # Dataset
    'dataset_size': 2000,
    'augmentation': True
}
```

## ğŸ“ˆ Monitoring

### **Automatische Plots:**
- Training/Validation Loss Curves
- Learning Rate Schedule
- Loss Gradients
- Overfitting Detection

### **Checkpoints:**
- `best_model.pth` - Bestes Validation Model
- `final_advanced_upscaler.pth` - Finales Model
- `checkpoint_epoch_X.pth` - Alle 10 Epochen

## ğŸ”§ Hardware Requirements

### **Minimum:**
- **GPU**: 6GB VRAM (GTX 1060, RTX 2060)
- **RAM**: 16GB
- **Storage**: 10GB fÃ¼r Datasets

### **Empfohlen:**
- **GPU**: 12GB+ VRAM (RTX 3080, RTX 4070)
- **RAM**: 32GB
- **Storage**: 50GB fÃ¼r groÃŸe Datasets

### **Batch Size Empfehlungen:**
- **6-8GB VRAM**: batch_size = 8
- **8-12GB VRAM**: batch_size = 16  
- **12GB+ VRAM**: batch_size = 32

## ğŸ“ Datei-Struktur

```
denrakeiw_nodes/
â”œâ”€â”€ advanced_trainer.py          # Trainer-Klassen
â”œâ”€â”€ dataset_preparation.py       # Dataset-Download & Prep
â”œâ”€â”€ train_advanced_upscaler.py   # Main Training Script
â”œâ”€â”€ quick_start_training.py      # Ein-Klick Setup
â”œâ”€â”€ wan_nn_latent_upscaler.py   # ComfyUI Node
â””â”€â”€ datasets/                    # Auto-erstellt
    â”œâ”€â”€ div2k/                   # DIV2K Dataset
    â”œâ”€â”€ latents/                 # Kodierte Latents
    â”‚   â”œâ”€â”€ train/              # Training Latents
    â”‚   â””â”€â”€ validation/         # Validation Latents
    â””â”€â”€ dataset_info.json       # Dataset Info
```

## ğŸ® Nach dem Training

### **1. Model in ComfyUI verwenden:**
```bash
# Kopiere bestes Model
cp models/best_model.pth /path/to/ComfyUI/models/upscale_models/

# Starte ComfyUI neu
```

### **2. Node verwenden:**
- Suche nach "Universal Latent Upscaler"
- Verbinde Latent Input â†’ Node â†’ Latent Output
- GenieÃŸe 2x bessere QualitÃ¤t! ğŸš€

## ğŸ”¬ Erweiterte Optionen

### **Custom Dataset hinzufÃ¼gen:**
```python
# Eigene Bilder hinzufÃ¼gen
creator = LatentDatasetCreator()
creator.encoder.encode_directory(
    "my_images/", 
    "datasets/latents/train/"
)
```

### **Training fortsetzen:**
```python
# Lade Checkpoint
checkpoint = torch.load("models/checkpoint_epoch_50.pth")
model.load_state_dict(checkpoint['model_state_dict'])
optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
```

### **Hyperparameter-Tuning:**
```python
# Experimentiere mit:
- num_residual_blocks: 4, 6, 8, 12
- learning_rate: 1e-5, 5e-5, 1e-4, 2e-4  
- loss_weights: (0.8, 0.2), (0.6, 0.4)
- batch_size: 8, 16, 32
```

## ğŸ› Troubleshooting

### **CUDA Out of Memory:**
```python
# Reduziere batch_size
config['batch_size'] = 8

# Oder verwende Gradient Accumulation
config['accumulate_grad_batches'] = 2
```

### **Slow Training:**
```python
# ErhÃ¶he num_workers
config['num_workers'] = 8

# Verwende pin_memory
pin_memory=True
```

### **Poor Quality:**
```python
# ErhÃ¶he Perceptual Loss Weight
config['perceptual_weight'] = 0.5

# Mehr Residual Blocks
config['num_residual_blocks'] = 12

# LÃ¤ngeres Training
config['epochs'] = 300
```

## ğŸ“Š Erwartete Ergebnisse

### **Nach 50 Epochen:**
- Grundlegende Upscaling-FÃ¤higkeit
- Reduzierte Artefakte

### **Nach 100 Epochen:**
- Gute Detail-Rekonstruktion
- Stabile Farben

### **Nach 200 Epochen:**
- Hochqualitative Ergebnisse
- Bessere SchÃ¤rfe als Standard-Upscaling

## ğŸ‰ Support

Bei Fragen oder Problemen:
1. PrÃ¼fe die Logs in `logs/`
2. Schaue dir die Plots in `plots/` an
3. Teste verschiedene Hyperparameter

**Viel Erfolg beim Training! ğŸš€**
